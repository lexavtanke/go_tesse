{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexpan/miniconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/alexpan/miniconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/alexpan/miniconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/alexpan/miniconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/alexpan/miniconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/alexpan/miniconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/alexpan/miniconda3/envs/goseek/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/alexpan/miniconda3/envs/goseek/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/alexpan/miniconda3/envs/goseek/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/alexpan/miniconda3/envs/goseek/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/alexpan/miniconda3/envs/goseek/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/alexpan/miniconda3/envs/goseek/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from gym import spaces\n",
    "from stable_baselines.common.policies import CnnLnLstmPolicy, LstmPolicy\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "from tesse.msgs import *\n",
    "import time\n",
    "\n",
    "from tesse_gym import get_network_config\n",
    "from tesse_gym.tasks.goseek import GoSeekFullPerception\n",
    "\n",
    "from tesse.msgs import Camera, Channels, Compression, DataRequest, DataResponse, ObjectsRequest, RemoveObjectsRequest\n",
    "from tesse_gym.tasks.goseek.goseek import GoSeek\n",
    "\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, Union\n",
    "\n",
    "import tensorflow as tf\n",
    "from stable_baselines.common.policies import nature_cnn\n",
    "\n",
    "from stable_baselines.common.callbacks import CheckpointCallback\n",
    "\n",
    "from tesse_gym.core.utils import set_all_camera_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update simulator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update expected observation shape\n",
    "class GoSeekUpdatedResolution(GoSeekFullPerception):\n",
    "    shape = (120, 160, 5)\n",
    "\n",
    "    @property\n",
    "    def observation_space(self) -> spaces.Box:\n",
    "        \"\"\" Define an observation space for RGB, depth, segmentation, and pose.\n",
    "\n",
    "       Because Stables Baselines (the baseline PPO library) does not support dictionary spaces,\n",
    "       the observation images and pose vector will be combined into a vector. The RGB image\n",
    "       is of shape (240, 320, 3), depth and segmentation are both (240, 320), ose is (3,), thus\n",
    "       the total shape is (240 * 320 * 5 + 3).\n",
    "       \"\"\"\n",
    "        return spaces.Box(np.Inf, np.Inf, shape=(120 * 160 * 5 + 3,))\n",
    "\n",
    "    def form_agent_observation(self, tesse_data: DataResponse) -> np.ndarray:\n",
    "        \"\"\" Create the agent's observation from a TESSE data response.\n",
    "\n",
    "        Args:\n",
    "            tesse_data (DataResponse): TESSE DataResponse object containing\n",
    "                RGB, depth, segmentation, and pose.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The agent's observation consisting of flatted RGB,\n",
    "                segmentation, and depth images concatenated with the relative\n",
    "                pose vector. To recover images and pose, see `decode_observations` below.\n",
    "        \"\"\"\n",
    "        eo, seg, depth = tesse_data.images\n",
    "        seg = seg[..., 0].copy()  # get segmentation as one-hot encoding\n",
    "\n",
    "        # See WALL_CLS comment\n",
    "        seg[seg > (self.N_CLASSES - 1)] = self.WALL_CLS\n",
    "        observation = np.concatenate(\n",
    "            (\n",
    "                eo / 255.0,\n",
    "                seg[..., np.newaxis] / (self.N_CLASSES - 1),\n",
    "                depth[..., np.newaxis],\n",
    "            ),\n",
    "            axis=-1,\n",
    "        ).reshape(-1)\n",
    "        pose = self.get_pose().reshape((3))\n",
    "        # print(\"_______________________________\")\n",
    "        # print(\"OBSERVATIO SHAPe \", observation.shape)\n",
    "        # print(\"pose shape \", pose.shape)\n",
    "\n",
    "\n",
    "        img_shape = (-1, 120, 160, 5)\n",
    "        #     print('observation shape', observation.shape)\n",
    "        # observation = np.expand_dims(observation, axis=0)\n",
    "        imgs = observation.reshape(img_shape)\n",
    "        #     print('image shape', imgs.shape)\n",
    "        rgb = imgs[..., :3]\n",
    "        #     print('rgb shape', rgb.shape)\n",
    "        gray = np.dot(rgb[..., :3], np.array([0.2989, 0.5870, 0.1140]).reshape(-1, 1))\n",
    "        #     print('gray shape', gray.shape)\n",
    "        segmentation = imgs[..., 3]\n",
    "        segmentation = np.expand_dims(segmentation, axis=3)\n",
    "        #     print('segmentation shape', segmentation.shape)\n",
    "        masked_fruit = np.ma.masked_values(segmentation == 1.0, segmentation)\n",
    "        #     masked_fruit = np.expand_dims(masked_fruit, axis=3)\n",
    "        # print('masked_fruit', masked_fruit.shape)\n",
    "        masked_furniture = np.ma.masked_inside(segmentation, 0.5, 1.0)\n",
    "        #     masked_furniture = np.expand_dims(masked_furniture, axis=3)\n",
    "        depth = imgs[..., 4]\n",
    "        depth = np.expand_dims(depth, axis=3)\n",
    "        #     print('depth shape', depth.shape)\n",
    "\n",
    "        imgs = np.concatenate((gray, segmentation, masked_fruit, masked_furniture, depth), axis=-1)\n",
    "        #     print('new imgs shape ', imgs.shape)\n",
    "        observation = imgs.reshape((img_shape[1] * img_shape[2] * img_shape[3]))\n",
    "        # print(\"observ shape before concat \", observation.shape)\n",
    "        return np.concatenate((observation, pose))\n",
    "\n",
    "    def compute_reward(\n",
    "            self, observation: DataResponse, action: int\n",
    "    ) -> Tuple[float, Dict[str, Union[int, bool]]]:\n",
    "        targets = self.env.request(ObjectsRequest())\n",
    "        \"\"\" Compute reward.\n",
    "\n",
    "        Reward consists of:\n",
    "            - Small time penalty\n",
    "            - # penalty for too near objects \n",
    "            - n_targets_found * `target_found_reward` if `action` == 3.\n",
    "                n_targets_found is the number of targets that are\n",
    "                (1) within `success_dist` of agent and (2) within\n",
    "                a bearing of `CAMERA_FOV` degrees.\n",
    "\n",
    "        Args:\n",
    "            observation (DataResponse): TESSE DataResponse object containing images\n",
    "                and metadata.\n",
    "            action (int): Action taken by agent.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, dict[str, [bool, int]]\n",
    "                Reward\n",
    "                Dictionary with the following keys\n",
    "                    - env_changed: True if agent changed the environment.\n",
    "                    - collision: True if there was a collision\n",
    "                    - n_found_targets: Number of targets found during step.\n",
    "        \"\"\"\n",
    "        # If not in ground truth mode, metadata will only provide position estimates\n",
    "        # In that case, get ground truth metadata from the controller\n",
    "        agent_metadata = (\n",
    "            observation.metadata\n",
    "            if self.ground_truth_mode\n",
    "            else self.continuous_controller.get_broadcast_metadata()\n",
    "        )\n",
    "        reward_info = {\"env_changed\": False, \"collision\": False, \"n_found_targets\": 0}\n",
    "\n",
    "        # compute agent's distance from targets\n",
    "        agent_position = self._get_agent_position(agent_metadata)\n",
    "        target_ids, target_position = self._get_target_id_and_positions(\n",
    "            targets.metadata\n",
    "        )\n",
    "\n",
    "        reward = -0.01 * self.target_found_reward  # small time penalty\n",
    "        # decode data by types\n",
    "        rgb, segmentation, depth, pose = self.extract_img(self.form_agent_observation(observation))\n",
    "        # penalty for too near objects\n",
    "        far_clip_plane = 50\n",
    "        # agent_observ = self.form_agent_observation(observation)\n",
    "        depth *= far_clip_plane  # convert depth to meters\n",
    "        # binary mask for obj nearly 0.7 m\n",
    "        masked_depth = np.ma.masked_values(depth <= 1.0, depth)\n",
    "        if np.count_nonzero(masked_depth) > 7000:\n",
    "            reward -= self.target_found_reward * 0.01\n",
    "        # get masked fruit from segmentation\n",
    "        masked_fruit = np.ma.masked_values(segmentation == 1.0, segmentation)\n",
    "        # penalty for get action without fruit in camera observation\n",
    "        size_masked_fruit = np.count_nonzero(masked_fruit)\n",
    "        # print(f\"fruit consists of {size_masked_fruit} points\")\n",
    "        if action == 3 and np.count_nonzero(masked_fruit) < 90:\n",
    "            # print(f\"sorry, you can't get it cause you don't see it\")\n",
    "            reward -= self.target_found_reward * 0.02\n",
    "\n",
    "        # check for found targets\n",
    "        if target_position.shape[0] > 0 and action == 3:\n",
    "            found_targets = self.get_found_targets(\n",
    "                agent_position, target_position, target_ids, agent_metadata\n",
    "            )\n",
    "\n",
    "            # if targets are found, update reward and related episode info\n",
    "            if len(found_targets):\n",
    "                self.n_found_targets += len(found_targets)\n",
    "                relative_pose = self.get_pose()\n",
    "                # relative_distanse = math.sqrt(relative_pose[0]**2 + relative_pose[1]**2)\n",
    "                fruit_position_bonus = math.sqrt(\n",
    "                    relative_pose[0] ** 2 + relative_pose[1] ** 2) * self.target_found_reward * 0.02\n",
    "                # print(f\"position fruit bonus is {fruit_position_bonus} relative distanse is {relative_distanse}\")\n",
    "                reward += self.target_found_reward * len(found_targets) + \\\n",
    "                          self.n_found_targets * self.target_found_reward * 0.02 + fruit_position_bonus\n",
    "\n",
    "                self.env.request(RemoveObjectsRequest(ids=found_targets))\n",
    "                reward_info[\"env_changed\"] = True\n",
    "                reward_info[\"n_found_targets\"] += len(found_targets)\n",
    "\n",
    "                # if all targets have been found, restart the episode\n",
    "                if self.n_found_targets == self.n_targets:\n",
    "                    self.done = True\n",
    "            else:\n",
    "                reward -= self.target_found_reward * 0.02\n",
    "\n",
    "        self.steps += 1\n",
    "        if self.steps > self.episode_length:\n",
    "            # square = self.getSquare()\n",
    "            # # reward for search new square\n",
    "            # if square < 340.0:\n",
    "            #     reward += 0.1 * square * self.target_found_reward\n",
    "            # self.positions.clear()\n",
    "            self.done = True\n",
    "\n",
    "        # collision information isn't provided by the controller metadata\n",
    "        if self._collision(observation.metadata):\n",
    "            reward_info[\"collision\"] = True\n",
    "            reward -= self.target_found_reward * 0.02\n",
    "\n",
    "            if self.restart_on_collision:\n",
    "                self.done = True\n",
    "        # else:\n",
    "        #     reward += self.target_found_reward * 0.005\n",
    "        # print(f\"reward for action {action} is {reward}\")\n",
    "        return reward, reward_info\n",
    "\n",
    "    def extract_img(\n",
    "            self, observation: np.ndarray, img_shape: Tuple[int, int, int, int] = (-1, 120, 160, 5)\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\" Decode observation vector into images and poses.\n",
    "\n",
    "        Args:\n",
    "            observation (np.ndarray): Shape (N,) observation array of flattened\n",
    "                images concatenated with a pose vector. Thus, N is equal to N*H*W*C + N*3.\n",
    "            img_shape (Tuple[int, int, int, int]): Shapes of all observed images stacked across\n",
    "                the channel dimension, resulting in a shape of (N, H, W, C).\n",
    "                 Default value is (-1, 240, 320, 5).\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: Arrays with the following information\n",
    "                - RGB image(s) of shape (N, H, W, 3)\n",
    "                - Segmentation image(s) of shape (N, H, W), in range [0, C) where C is the number of classes.\n",
    "                - Depth image(s) of shape (N, H, W), in range [0, 1]. To get true depth, multiply by the\n",
    "                    Unity far clipping plane (default 50).\n",
    "                - Pose array of shape (N, 3) containing (x, y, heading) relative to starting point.\n",
    "                    (x, y) are in meters, heading is given in degrees in the range [-180, 180].\n",
    "        \"\"\"\n",
    "        observation = np.expand_dims(observation, axis=0)\n",
    "        imgs = observation[:, :-3].reshape(img_shape)\n",
    "        rgb = imgs[..., :3]\n",
    "        segmentation = imgs[..., 3]\n",
    "        depth = imgs[..., 4]\n",
    "\n",
    "        pose = observation[:, -3:]\n",
    "\n",
    "        return rgb, segmentation, depth, pose\n",
    "\n",
    "    def getTriangle(self, x, y, tetha):\n",
    "        alpha = self.CAMERA_HFOV\n",
    "        radius = self.success_dist\n",
    "        x1 = x + (math.sin(math.radians(tetha + (alpha / 2))) * radius)\n",
    "        y1 = y + (math.cos(math.radians(tetha + (alpha / 2))) * radius)\n",
    "        x2 = x + (math.sin(math.radians(tetha - (alpha / 2))) * radius)\n",
    "        y2 = y + (math.cos(math.radians(tetha - (alpha / 2))) * radius)\n",
    "        return Polygon([(x, y), (x1, y1), (x2, y2)])\n",
    "\n",
    "    def getSquare(self):\n",
    "        polygons = []\n",
    "        for ar in self.positions:\n",
    "            poly = self.getTriangle(ar.item(0), ar.item(1), ar.item(2))\n",
    "            polygons.append(poly)\n",
    "\n",
    "        result = unary_union(polygons)\n",
    "        return result.area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update simulator cameras on init\n",
    "def set_resolution(tesse_gym):\n",
    "    set_all_camera_params(tesse_gym, height_in_pixels=120, width_in_pixels=160)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set sim path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path(\"../../goseek-challenge/simulator/goseek-v0.1.4.x86_64\")\n",
    "assert filename.exists(), f\"Must set a valid path!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set environment parameters\n",
    "\n",
    "\n",
    "__Note__ To minimize training time during initial use, we've set `total_timestamps` and `n_environments` to 1e5 and 2 respectively. Setting `total_timestamps` to 3e6 and `n_environments` to 4 should produce an agent that approximates our baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_environments = 2  # number of environments to train over\n",
    "total_timesteps = 100000  # number of training timesteps\n",
    "scene_id = [1, 2, 3, 4, 5, 5]  # list all available scenes\n",
    "n_targets = 30  # number of targets spawned in each scene\n",
    "target_found_reward = 3  # reward per found target\n",
    "episode_length = 400\n",
    "\n",
    "\n",
    "\n",
    "def make_unity_env(filename, num_env):\n",
    "    \"\"\" Create a wrapped Unity environment. \"\"\"\n",
    "\n",
    "    def make_env(rank):\n",
    "\n",
    "        def _thunk():\n",
    "            env = GoSeekUpdatedResolution(\n",
    "                str(filename),\n",
    "                network_config=get_network_config(worker_id=rank),\n",
    "                n_targets=n_targets,\n",
    "                episode_length=episode_length,\n",
    "                scene_id=scene_id[rank],\n",
    "                target_found_reward=target_found_reward,\n",
    "            )\n",
    "\n",
    "            return env\n",
    "\n",
    "        return _thunk\n",
    "\n",
    "    return SubprocVecEnv([make_env(i) for i in range(num_env)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = make_unity_env(filename, n_environments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Model \n",
    "\n",
    "The following network assumes an observation of consisting of RGB, segmentation, and depth images along with the agent's relative pose from start. Images are processed using the Stable Baseline default CNN. The resulting feature vector is concatenated with the pose vector and given to an LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from stable_baselines.common.policies import nature_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define network to consume images and pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tensor_observations(observation, img_shape=(-1, 120, 160, 5)):\n",
    "    \"\"\" Decode observation vector into images and poses.\n",
    "\n",
    "    Args:\n",
    "        observation (np.ndarray): Shape (N,) observation array of flattened\n",
    "            images concatenated with a pose vector. Thus, N is equal to N*H*W*C + N*3.\n",
    "        img_shape (Tuple[int, int, int, int]): Shapes of all images stacked in (N, H, W, C).\n",
    "            Default value is (-1, 240, 320, 5).\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[tf.Tensor, tf.Tensor]: Tensors with the following information\n",
    "            - Tensor of shape (N, `img_shape[1:]`) containing RGB,\n",
    "                segmentation, and depth images stacked across the channel dimension.\n",
    "            - Tensor of shape (N, 3) containing (x, y, heading) relative to starting point.\n",
    "                (x, y) are in meters, heading is given in degrees in the range [-180, 180].\n",
    "    \"\"\"\n",
    "    imgs = tf.reshape(observation[:, :-3], img_shape)\n",
    "    pose = observation[:, -3:]\n",
    "\n",
    "    return imgs, pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_and_pose_network(observation, **kwargs):\n",
    "    \"\"\" Network to process image and pose data.\n",
    "    \n",
    "    Use the stable baselines nature_cnn to process images. The resulting\n",
    "    feature vector is then combined with the pose estimate and given to an\n",
    "    LSTM (LSTM defined in PPO2 below).\n",
    "    \n",
    "    Args:\n",
    "        raw_observations (tf.Tensor): 1D tensor containing image and \n",
    "            pose data.\n",
    "        \n",
    "    Returns:\n",
    "        tf.Tensor: Feature vector. \n",
    "    \"\"\"\n",
    "    imgs, pose = decode_tensor_observations(observation)\n",
    "    image_features = nature_cnn(imgs)\n",
    "    return tf.concat((image_features, pose), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.tf_layers import conv, linear, conv_to_fc, lstm\n",
    "def image_and_pose_network2(observation, **kwargs):\n",
    "    imgs, pose = decode_tensor_observations(observation)\n",
    "    activ = tf.nn.relu\n",
    "    layer_1 = activ(conv(imgs, 'c1', n_filters=64, filter_size=8, stride=4, init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_2 = activ(conv(layer_1, 'c2', n_filters=128, filter_size=4, stride=2, init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_3 = activ(conv(layer_2, 'c3', n_filters=128, filter_size=3, stride=1, init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_3 = conv_to_fc(layer_3)\n",
    "    image_features =  activ(linear(layer_3, 'fc1', n_hidden=1024, init_scale=np.sqrt(2)))\n",
    "    return tf.concat((image_features, pose), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register custom network\n",
    "\n",
    "Outputs of the network defined above will be fed into an LSTM defined below in PPO2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = {'cnn_extractor': image_and_pose_network}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCnnLnLstmPolicy(LstmPolicy):\n",
    "\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, n_lstm=1024, reuse=False, **_kwargs):\n",
    "        super(CustomCnnLnLstmPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, n_lstm, reuse,\n",
    "                                              layer_norm=True, feature_extraction=\"cnn\", **_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alexpan/miniconda3/envs/goseek/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/alexpan/miniconda3/envs/goseek/lib/python3.7/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/alexpan/miniconda3/envs/goseek/lib/python3.7/site-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/alexpan/miniconda3/envs/goseek/lib/python3.7/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/alexpan/miniconda3/envs/goseek/lib/python3.7/site-packages/stable_baselines/common/tf_layers.py:103: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/alexpan/miniconda3/envs/goseek/lib/python3.7/site-packages/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/alexpan/miniconda3/envs/goseek/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/alexpan/miniconda3/envs/goseek/lib/python3.7/site-packages/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/alexpan/miniconda3/envs/goseek/lib/python3.7/site-packages/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = PPO2(\n",
    "    CustomCnnLnLstmPolicy,\n",
    "    env,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./tensorboard/\",\n",
    "    n_steps = 128,\n",
    "    nminibatches=2,\n",
    "    cliprange = 0.2,\n",
    "    gamma=0.999,\n",
    "    noptepochs = 3,\n",
    "    learning_rate=0.0003,\n",
    "#    full_tensorboard_log=True,\n",
    "    policy_kwargs=policy_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define logging directory and callback function to save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path(\"results/goseek-ppo\")\n",
    "log_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO2.load(str( f\"results/ppo2-newhyper4600k.pkl\"), env,verbose=1, tensorboard_log=\"./tensorboard/\",)\n",
    "#model.set_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.callbacks import CallbackList, CheckpointCallback\n",
    "checkpoint_callback = CheckpointCallback(save_freq=3000, save_path='./results/',\n",
    "                                         name_prefix='ppo2-5-NR5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/alexpan/miniconda3/envs/goseek/lib/python3.7/site-packages/stable_baselines/common/base_class.py:1143: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-dc5281afea1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./results/',\n\u001b[1;32m      3\u001b[0m                                          name_prefix='ppo2-stalin-bigmem-newhyper8')\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/goseek/lib/python3.7/site-packages/stable_baselines/ppo2/ppo2.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_rollout_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0;31m# true_reward is the reward without discount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0mrollout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                 \u001b[0;31m# Unpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneglogpacs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrollout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/goseek/lib/python3.7/site-packages/stable_baselines/common/base_class.py\u001b[0m in \u001b[0;36mrunner\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    792\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAbstractEnvRunner\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_runner\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_runner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_runner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_runner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/goseek/lib/python3.7/site-packages/stable_baselines/ppo2/ppo2.py\u001b[0m in \u001b[0;36m_make_runner\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_runner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         return Runner(env=self.env, model=self, n_steps=self.n_steps,\n\u001b[0;32m--> 100\u001b[0;31m                       gamma=self.gamma, lam=self.lam)\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_pretrain_placeholders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/goseek/lib/python3.7/site-packages/stable_baselines/ppo2/ppo2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, model, n_steps, gamma, lam)\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mFactor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrade\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0moff\u001b[0m \u001b[0mof\u001b[0m \u001b[0mbias\u001b[0m \u001b[0mvs\u001b[0m \u001b[0mvariance\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mGeneralized\u001b[0m \u001b[0mAdvantage\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \"\"\"\n\u001b[0;32m--> 449\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/goseek/lib/python3.7/site-packages/stable_baselines/common/runners.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, model, n_steps)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_ob_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_envs\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_envs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/goseek/lib/python3.7/site-packages/stable_baselines/common/vec_env/subproc_vec_env.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mremote\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremotes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mremote\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremotes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_flatten_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/goseek/lib/python3.7/site-packages/stable_baselines/common/vec_env/subproc_vec_env.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mremote\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremotes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'reset'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mremote\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremotes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_flatten_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/goseek/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/goseek/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/goseek/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"got end of file during message\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.callbacks import CheckpointCallback\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./results/',\n",
    "                                         name_prefix='ppo2-stalin-bigmem-newhyper8')\n",
    "model.learn(total_timesteps=total_timesteps,  callback=checkpoint_callback, reset_num_timesteps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(str( f\"results/ppo2-newhyper4700k.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results\n",
    "\n",
    "__Note__: Stable-Baselines requires that policy input dimensions be consistent across training and testing. Thus, the number of environments used for visualization must be a multiple of the number of environments used for training. The observation vector is then appropriately duplicated during inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_WEIGHTS_PATH = \"./results/ppo2-newhyper.pkl\"\n",
    "assert MODEL_WEIGHTS_PATH, f\"Must give a model weights path!\"\n",
    "\n",
    "#model = PPO2.load(str(MODEL_WEIGHTS_PATH))\n",
    "n_train_envs = model.act_model.initial_state.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize all observed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "rgb, segmentation, depth, pose = decode_observations(obs)\n",
    "lstm_state = None\n",
    "\n",
    "assert (\n",
    "    n_train_envs % obs.shape[0] == 0\n",
    "), f\"The number of visualization environments must be a multiple of the training environments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "print(segmentation[0])\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(rgb[0])\n",
    "ax[1].imshow(segmentation[0])\n",
    "ax[2].imshow(depth[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run an episode and plot the first person agent view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "done = False\n",
    "fig, ax = plt.subplots(1, obs.shape[0])\n",
    "ax = [ax] if obs.shape[0] == 1 else ax\n",
    "\n",
    "for i in range(episode_length):\n",
    "    actions, lstm_state = model.predict(\n",
    "        np.concatenate((n_train_envs // obs.shape[0]) * [obs]),\n",
    "        state=lstm_state,\n",
    "        deterministic=False,\n",
    "    )\n",
    "\n",
    "    actions = actions[: obs.shape[0]]\n",
    "    obs, reward, done, _ = env.step(actions)\n",
    "\n",
    "    plt.cla()\n",
    "    rgb, segmentation, depth, pose = decode_observations(obs)\n",
    "\n",
    "    for i in range(obs.shape[0]):\n",
    "        ax[i].imshow(rgb[i])\n",
    "    fig.canvas.draw()\n",
    "\n",
    "obs = env.reset()\n",
    "rgb, segmentation, depth, pose = decode_observations(obs)\n",
    "lstm_state = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "goseek",
   "language": "python",
   "name": "goseek"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
