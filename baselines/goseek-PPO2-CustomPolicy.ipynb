{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from gym import spaces\n",
    "from stable_baselines.common.policies import CnnLnLstmPolicy, LstmPolicy\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "from tesse.msgs import *\n",
    "import time\n",
    "\n",
    "from tesse_gym import get_network_config\n",
    "from tesse_gym.tasks.goseek import GoSeekFullPerception, decode_observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set sim path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path(\"../../goseek-challenge/simulator/goseek-v0.1.4.x86_64\")\n",
    "assert filename.exists(), f\"Must set a valid path!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set environment parameters\n",
    "\n",
    "\n",
    "__Note__ To minimize training time during initial use, we've set `total_timestamps` and `n_environments` to 1e5 and 2 respectively. Setting `total_timestamps` to 3e6 and `n_environments` to 4 should produce an agent that approximates our baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_environments = 6  # number of environments to train over\n",
    "total_timesteps = 100000  # number of training timesteps\n",
    "scene_id = [1, 2, 3, 4, 5, 5]  # list all available scenes\n",
    "n_targets = 30  # number of targets spawned in each scene\n",
    "target_found_reward = 3  # reward per found target\n",
    "episode_length = 400\n",
    "\n",
    "\n",
    "\n",
    "def make_unity_env(filename, num_env):\n",
    "    \"\"\" Create a wrapped Unity environment. \"\"\"\n",
    "\n",
    "    def make_env(rank):\n",
    "\n",
    "        def _thunk():\n",
    "            env = GoSeekFullPerception(\n",
    "                str(filename),\n",
    "                network_config=get_network_config(worker_id=rank),\n",
    "                n_targets=n_targets,\n",
    "                episode_length=episode_length,\n",
    "                scene_id=scene_id[rank],\n",
    "                target_found_reward=target_found_reward,\n",
    "            )\n",
    "\n",
    "            return env\n",
    "\n",
    "        return _thunk\n",
    "\n",
    "    return SubprocVecEnv([make_env(i) for i in range(num_env)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = make_unity_env(filename, n_environments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Model \n",
    "\n",
    "The following network assumes an observation of consisting of RGB, segmentation, and depth images along with the agent's relative pose from start. Images are processed using the Stable Baseline default CNN. The resulting feature vector is concatenated with the pose vector and given to an LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from stable_baselines.common.policies import nature_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define network to consume images and pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tensor_observations(observation, img_shape=(-1, 240, 320, 5)):\n",
    "    \"\"\" Decode observation vector into images and poses.\n",
    "\n",
    "    Args:\n",
    "        observation (np.ndarray): Shape (N,) observation array of flattened\n",
    "            images concatenated with a pose vector. Thus, N is equal to N*H*W*C + N*3.\n",
    "        img_shape (Tuple[int, int, int, int]): Shapes of all images stacked in (N, H, W, C).\n",
    "            Default value is (-1, 240, 320, 5).\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[tf.Tensor, tf.Tensor]: Tensors with the following information\n",
    "            - Tensor of shape (N, `img_shape[1:]`) containing RGB,\n",
    "                segmentation, and depth images stacked across the channel dimension.\n",
    "            - Tensor of shape (N, 3) containing (x, y, heading) relative to starting point.\n",
    "                (x, y) are in meters, heading is given in degrees in the range [-180, 180].\n",
    "    \"\"\"\n",
    "    imgs = tf.reshape(observation[:, :-3], img_shape)\n",
    "    pose = observation[:, -3:]\n",
    "\n",
    "    return imgs, pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_and_pose_network(observation, **kwargs):\n",
    "    \"\"\" Network to process image and pose data.\n",
    "    \n",
    "    Use the stable baselines nature_cnn to process images. The resulting\n",
    "    feature vector is then combined with the pose estimate and given to an\n",
    "    LSTM (LSTM defined in PPO2 below).\n",
    "    \n",
    "    Args:\n",
    "        raw_observations (tf.Tensor): 1D tensor containing image and \n",
    "            pose data.\n",
    "        \n",
    "    Returns:\n",
    "        tf.Tensor: Feature vector. \n",
    "    \"\"\"\n",
    "    imgs, pose = decode_tensor_observations(observation)\n",
    "    image_features = nature_cnn(imgs)\n",
    "    return tf.concat((image_features, pose), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.tf_layers import conv, linear, conv_to_fc, lstm\n",
    "def image_and_pose_network2(observation, **kwargs):\n",
    "    imgs, pose = decode_tensor_observations(observation)\n",
    "    activ = tf.nn.relu\n",
    "    layer_1 = activ(conv(imgs, 'c1', n_filters=64, filter_size=8, stride=4, init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_2 = activ(conv(layer_1, 'c2', n_filters=128, filter_size=4, stride=2, init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_3 = activ(conv(layer_2, 'c3', n_filters=128, filter_size=3, stride=1, init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_3 = conv_to_fc(layer_3)\n",
    "    image_features =  activ(linear(layer_3, 'fc1', n_hidden=1024, init_scale=np.sqrt(2)))\n",
    "    return tf.concat((image_features, pose), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register custom network\n",
    "\n",
    "Outputs of the network defined above will be fed into an LSTM defined below in PPO2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = {'cnn_extractor': image_and_pose_network}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCnnLnLstmPolicy(LstmPolicy):\n",
    "\n",
    "    def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, n_lstm=1024, reuse=False, **_kwargs):\n",
    "        super(CustomCnnLnLstmPolicy, self).__init__(sess, ob_space, ac_space, n_env, n_steps, n_batch, n_lstm, reuse,\n",
    "                                              layer_norm=True, feature_extraction=\"cnn\", **_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO2(\n",
    "    CustomCnnLnLstmPolicy,\n",
    "    env,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./tensorboard/\",\n",
    "    n_steps = 128,\n",
    "    nminibatches=6,\n",
    "    cliprange = 0.2,\n",
    "    gamma=0.999,\n",
    "    noptepochs = 3,\n",
    "    learning_rate=0.0003,\n",
    "#    full_tensorboard_log=True,\n",
    "    policy_kwargs=policy_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define logging directory and callback function to save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path(\"results/goseek-ppo\")\n",
    "log_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO2.load(str( f\"results/ppo2-newhyper4600k.pkl\"), env,verbose=1, tensorboard_log=\"./tensorboard/\",)\n",
    "#model.set_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.callbacks import CallbackList, CheckpointCallback\n",
    "checkpoint_callback = CheckpointCallback(save_freq=3000, save_path='./results/',\n",
    "                                         name_prefix='ppo2-5-NR5')\n",
    "\n",
    "callbackList = CallbackList([checkpoint_callback, TensorboardCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.callbacks import CheckpointCallback\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1000, save_path='./results/',\n",
    "                                         name_prefix='ppo2-stalin-bigmem-newhyper8')\n",
    "model.learn(total_timesteps=total_timesteps,  callback=checkpoint_callback, reset_num_timesteps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(str( f\"results/ppo2-newhyper4700k.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results\n",
    "\n",
    "__Note__: Stable-Baselines requires that policy input dimensions be consistent across training and testing. Thus, the number of environments used for visualization must be a multiple of the number of environments used for training. The observation vector is then appropriately duplicated during inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_WEIGHTS_PATH = \"./results/ppo2-newhyper.pkl\"\n",
    "assert MODEL_WEIGHTS_PATH, f\"Must give a model weights path!\"\n",
    "\n",
    "#model = PPO2.load(str(MODEL_WEIGHTS_PATH))\n",
    "n_train_envs = model.act_model.initial_state.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize all observed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "rgb, segmentation, depth, pose = decode_observations(obs)\n",
    "lstm_state = None\n",
    "\n",
    "assert (\n",
    "    n_train_envs % obs.shape[0] == 0\n",
    "), f\"The number of visualization environments must be a multiple of the training environments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "print(segmentation[0])\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(rgb[0])\n",
    "ax[1].imshow(segmentation[0])\n",
    "ax[2].imshow(depth[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run an episode and plot the first person agent view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "done = False\n",
    "fig, ax = plt.subplots(1, obs.shape[0])\n",
    "ax = [ax] if obs.shape[0] == 1 else ax\n",
    "\n",
    "for i in range(episode_length):\n",
    "    actions, lstm_state = model.predict(\n",
    "        np.concatenate((n_train_envs // obs.shape[0]) * [obs]),\n",
    "        state=lstm_state,\n",
    "        deterministic=False,\n",
    "    )\n",
    "\n",
    "    actions = actions[: obs.shape[0]]\n",
    "    obs, reward, done, _ = env.step(actions)\n",
    "\n",
    "    plt.cla()\n",
    "    rgb, segmentation, depth, pose = decode_observations(obs)\n",
    "\n",
    "    for i in range(obs.shape[0]):\n",
    "        ax[i].imshow(rgb[i])\n",
    "    fig.canvas.draw()\n",
    "\n",
    "obs = env.reset()\n",
    "rgb, segmentation, depth, pose = decode_observations(obs)\n",
    "lstm_state = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
